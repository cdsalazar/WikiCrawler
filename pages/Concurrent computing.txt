


		
		
		
			

							
						

			Concurrent computing
									
									From Wikipedia, the free encyclopedia
								
												
					Jump to:					navigation, 					search
				
				For a more theoretical discussion, see Concurrency (computer science).


Programming paradigms




Action
Agent-oriented
Array-oriented
Automata-based
Concurrent computing

Relativistic programming


Data-driven
Declarative (contrast: Imperative)

Constraint

Constraint logic

Concurrent constraint logic




Dataflow

Flow-based
Cell-oriented (spreadsheets)
Reactive


Functional

Functional logic
Purely functional


Logic

Abductive logic
Answer set
Concurrent logic
Functional logic
Inductive logic




End-user programming
Event-driven

Service-oriented
Time-driven


Expression-oriented
Feature-oriented
Function-level (contrast: Value-level)
Generic
Imperative (contrast: Declarative)

Literate
Procedural


Inductive programming
Language-oriented

Natural language programming
Discipline-specific
Domain-specific
Grammar-oriented

Dialecting


Intentional


Metaprogramming

Automatic
Reflective

Attribute-oriented


Homoiconic
Template

Policy-based




Non-structured (contrast: Structured)

Array


Nondeterministic
Parallel computing

Process-oriented


Point-free style

Concatenative


Semantic
Structured (contrast: Non-structured)

Block-structured
Modular (contrast: Monolithic)
Object-oriented (OOP)

Class-based
Concurrent
Prototype-based
By separation of concerns:

Aspect-oriented
Role-oriented
Subject-oriented




Recursive


Value-level (contrast: Function-level)
Probabilistic
Concept







v
t
e





Concurrent computing is a form of computing in which several computations are executing during overlapping time periods—concurrently—instead of sequentially (one completing before the next starts). This is a property of a system—this may be an individual program, a computer, or a network—and there is a separate execution point or "thread of control" for each computation ("process"). A concurrent system is one where a computation can make progress without waiting for all other computations to complete—where more than one computation can make progress at "the same time".[1]
As a programming paradigm, concurrent computing is a form of modular programming, namely factoring an overall computation into subcomputations that may be executed concurrently. Pioneers in the field of concurrent computing include Edsger Dijkstra, Per Brinch Hansen, and C.A.R. Hoare.



Contents


1 Introduction

1.1 Coordinating access to shared resources
1.2 Advantages


2 Models
3 Implementation

3.1 Interaction and communication


4 History
5 Prevalence
6 Languages supporting it
7 See also
8 Notes
9 References
10 Further reading
11 External links



Introduction[edit]
See also: Parallel computing
Concurrent computing is related to but distinct from parallel computing, though these concepts are frequently confused,[2][3] and both can be described as "multiple processes executing during the same period of time". In parallel computing, execution literally occurs at the same instant, for example on separate processors of a multi-processor machine, with the goal of speeding up computations—parallel computing is impossible on a (single-core) single processor, as only one computation can occur at any instant (during any single clock cycle).[a] By contrast, concurrent computing consists of process lifetimes overlapping, but execution need not happen at the same instant. The goal here is to model processes in the outside world that happen concurrently, such as multiple clients accessing a server at the same time. Structuring software systems as composed of multiple concurrent, communicating parts can be useful for tackling complexity, regardless of whether the parts can be executed in parallel.[4]:1
For example, concurrent processes can be executed on a single core by interleaving the execution steps of each process via time slices: only one process runs at a time, and if it does not complete during its time slice, it is paused, another process begins or resumes, and then later the original process is resumed. In this way multiple processes are part-way through execution at a single instant, but only one process is being executed at that instant.
Concurrent computations may be executed in parallel,[2][5] for example by assigning each process to a separate processor or processor core, or distributing a computation across a network, but in general, the languages, tools and techniques for parallel programming may not be suitable for concurrent programming, and vice versa.
The exact timing of when tasks in a concurrent system are executed depend on the scheduling, and tasks need not always be executed concurrently. For example, given two tasks, T1 and T2:

T1 may be executed and finished before T2 or vice versa (serial and sequential);
T1 and T2 may be executed alternately (serial and concurrent);
T1 and T2 may be executed simultaneously at the same instant of time (parallel and concurrent).

The word "sequential" is used as an antonym for both "concurrent" and "parallel"; when these are explicitly distinguished, concurrent/sequential and parallel/serial are used as opposing pairs.[6] A schedule in which tasks execute one at a time (serially, no parallelism), without interleaving (sequentually, no concurrency: no task begins until the previous task ends) is called a serial schedule. A set of tasks that can be scheduled serially is serializable, which simplifies concurrency control.
Coordinating access to shared resources[edit]
The main challenge in designing concurrent programs is concurrency control: ensuring the correct sequencing of the interactions or communications between different computational executions, and coordinating access to resources that are shared among executions.[5] Potential problems include race conditions, deadlocks, and resource starvation. For example, consider the following algorithm for making withdrawals from a checking account represented by the shared resource balance:


1 bool withdraw(int withdrawal)
2 {
3     if(balance &gt;= withdrawal)
4     {
5         balance -= withdrawal;
6         return true;
7     } 
8     return false;
9 }

Suppose balance = 500, and two concurrent threads make the calls withdraw(300) and withdraw(350). If line 3 in both operations executes before line 5 both operations will find that balance &gt;= withdrawal evaluates to true, and execution will proceed to subtracting the withdrawal amount. However, since both processes perform their withdrawals, the total amount withdrawn will end up being more than the original balance. These sorts of problems with shared resources require the use of concurrency control, or non-blocking algorithms.
Because concurrent systems rely on the use of shared resources (including communication media), concurrent computing in general requires the use of some form of arbiter somewhere in the implementation to mediate access to these resources.
Unfortunately, while many solutions exist to the problem of a conflict over one resource, many of those "solutions" have their own concurrency problems such as deadlock when more than one resource is involved.
Advantages[edit]





This section does not cite any references (sources). Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2006)



Increased application throughput—parallel execution of a concurrent program allows the number of tasks completed in certain time period to increase.
High responsiveness for input/output—input/output-intensive applications mostly wait for input or output operations to complete. Concurrent programming allows the time that would be spent waiting to be used for another task.
More appropriate program structure—some problems and problem domains are well-suited to representation as concurrent tasks or processes.

Models[edit]
There are several models of concurrent computing, which can be used to understand and analyze concurrent systems. These models include:

Actor model

Object-capability model for security


Petri nets
Process calculi such as

Ambient calculus
Calculus of Communicating Systems (CCS)
Communicating Sequential Processes (CSP)
π-calculus
Join-calculus


Input/output automaton

Implementation[edit]



This section requires expansion. (February 2014)


A number of different methods can be used to implement concurrent programs, such as implementing each computational execution as an operating system process, or implementing the computational processes as a set of threads within a single operating system process.
Interaction and communication[edit]
In some concurrent computing systems, communication between the concurrent components is hidden from the programmer (e.g., by using futures), while in others it must be handled explicitly. Explicit communication can be divided into two classes:

Shared memory communication
Concurrent components communicate by altering the contents of shared memory locations (exemplified by Java and C#). This style of concurrent programming usually requires the application of some form of locking (e.g., mutexes, semaphores, or monitors) to coordinate between threads. A program that properly implements any of these is said to be thread-safe.


Message passing communication
Concurrent components communicate by exchanging messages (exemplified by Scala, Erlang and occam). The exchange of messages may be carried out asynchronously, or may use a synchronous "rendezvous" style in which the sender blocks until the message is received. Asynchronous message passing may be reliable or unreliable (sometimes referred to as "send and pray"). Message-passing concurrency tends to be far easier to reason about than shared-memory concurrency, and is typically considered a more robust form of concurrent programming.[citation needed] A wide variety of mathematical theories for understanding and analyzing message-passing systems are available, including the Actor model, and various process calculi. Message passing can be efficiently implemented on symmetric multiprocessors, with or without shared coherent memory.

Shared memory and message passing concurrency have different performance characteristics. Typically (although not always), the per-process memory overhead and task switching overhead is lower in a message passing system, but the overhead of message passing itself is greater than for a procedure call. These differences are often overwhelmed by other performance factors.
History[edit]
Concurrent computing developed out of earlier work on railroads and telegraphy, from the 19th and early 20th century, and some terms date to this period, such as semaphores. These arose to address the question of how to handle multiple trains on the same railroad system (avoiding collisions and maximizing efficiency) and how to handle multiple transmissions over a given set of wires (improving efficiency), such as via time-division multiplexing (1870s).
The academic study of concurrent algorithms started in the 1960s, with Dijkstra (1965) credited with being the first paper in this field, identifying and solving mutual exclusion.[7]
Prevalence[edit]
Concurrency is pervasive in computing, occurring from low-level hardware on a single chip to world-wide networks. Examples follow.
At the programming language level:

Channel
Coroutine
Futures and promises

At the operating system level:

Computer multitasking, including both cooperative multitasking and preemptive multitasking

Time-sharing, which replaced sequential batch processing of jobs with concurrent use of a system


Process
Thread

At the network level, networked systems are generally concurrent by their nature, as they consist of separate devices.
Languages supporting it[edit]
Concurrent programming languages are programming languages that use language constructs for concurrency. These constructs may involve multi-threading, support for distributed computing, message passing, shared resources (including shared memory) or futures and promises. Such languages are sometimes described as Concurrency Oriented Languages or Concurrency Oriented Programming Languages (COPL).[8]
Today, the most commonly used programming languages that have specific constructs for concurrency are Java and C#. Both of these languages fundamentally use a shared-memory concurrency model, with locking provided by monitors (although message-passing models can and have been implemented on top of the underlying shared-memory model). Of the languages that use a message-passing concurrency model, Erlang is probably the most widely used in industry at present.[citation needed]
Many concurrent programming languages have been developed more as research languages (e.g. Pict) rather than as languages for production use. However, languages such as Erlang, Limbo, and occam have seen industrial use at various times in the last 20 years. Languages in which concurrency plays an important role include:

Ada—general purpose, with native support for message passing and monitor based concurrency
Alef—concurrent, with threads and message passing, for system programming in early versions of Plan 9 from Bell Labs
Alice—extension to Standard ML, adds support for concurrency via futures
Ateji PX—extension to Java with parallel primitives inspired from π-calculus
Axum—domain specific, concurrent, based on Actor model and .NET Common Language Runtime using a C-like syntax
C++—std::thread
Cω (C omega)—for research, extends C#, uses asynchronous communication
C#—supports concurrent computing since version 5.0 using lock, yield, async and await keywords
Clojure—modern Lisp for the JVM
Concurrent Clean—functional programming, similar to Haskell
Concurrent Collections (CnC)—Achieves implicit parallelism independent of memory model by explicitly defining flow of data and control
Concurrent Haskell—lazy, pure functional language operating concurrent processes on shared memory
Concurrent ML—concurrent extension of Standard ML
Concurrent Pascal—by Per Brinch Hansen
Curry
D—multi-paradigm system programming language with explicit support for concurrent programming (Actor model)
E—uses promises to disallow deadlocks
ECMAScript—promises available in various libraries, proposed for inclusion in standard in ECMAScript 6
Eiffel—through its SCOOP mechanism based on the concepts of Design by Contract
Elixir—dynamic and functional meta-programming aware language running on the Erlang VM.
Erlang—uses asynchronous message passing with nothing shared
FAUST—real-time functional, for signal processing, compiler provides automatic parallelization via OpenMP or a specific work-stealing scheduler
Fortran—coarrays and do concurrent are part of Fortran 2008 standard
Go—for system programming, with a concurrent programming model based on CSP
Hume—functional, concurrent, for bounded space and time environments where automata processes are described by synchronous channels patterns and message passing
Io—actor-based concurrency
Janus—features distinct askers and tellers to logical variables, bag channels; is purely declarative
JavaScript—via web workers, in a browser environment, promises, and callbacks.
JoCaml—concurrent and distributed channel based, extension of OCaml, implements the Join-calculus of processes
Join Java—concurrent, based on Java language
Joule—dataflow-based, communicates by message passing
Joyce—concurrent, teaching, built on Concurrent Pascal with features from CSP by Per Brinch Hansen
LabVIEW—graphical, dataflow, functions are nodes in a graph, data is wires between the nodes; includes object-oriented language
Limbo—relative of Alef, for system programming in Inferno (operating system)
MultiLisp—Scheme variant extended to support parallelism
Modula-2—for system programming, by N. Wirth as a successor to Pascal with native support for coroutines
Modula-3—modern member of Algol family with extensive support for threads, mutexes, condition variables
Newsqueak—for research , with channels as first-class values; predecessor of Alef
Node.js—a server-side runtime environment for JavaScript
occam—influenced heavily by Communicating Sequential Processes (CSP)

occam-π—a modern variant of occam, which incorporates ideas from Milner's π-calculus


Orc—heavily concurrent, nondeterministic, based on Kleene algebra
Oz—multiparadigm, supports shared-state and message-passing concurrency, and futures

Mozart Programming System—multiplatform Oz


ParaSail—object-oriented, parallel, free of pointers, race conditions
Pict—essentially an executable implementation of Milner's π-calculus
Perl with AnyEvent and Coro
Python with Twisted, greenlet and gevent
Reia—uses asynchronous message passing between shared-nothing objects
Red/System—for system programming, based on Rebol
Rust—for system programming, focus on massive concurrency, using message-passing with move semantics, shared immutable memory, and shared mutable memory that is provably free of race conditions.[9]
SALSA—actor-based with token-passing, join, and first-class continuations for distributed computing over the Internet
Scala—general purpose, designed to express common programming patterns in a concise, elegant, and type-safe way
SequenceL—general purpose functional, main design objectives are ease of programming, code clarity-readability, and automatic parallelization for performance on multicore hardware, and provably free of race conditions
SR—for research
Stackless Python
StratifiedJS—combinator-based concurrency, based on JavaScript
SuperPascal—concurrent, for teaching, built on Concurrent Pascal and Joyce by Per Brinch Hansen
Unicon—for research
Termite Scheme—adds Erlang-like concurrency to Scheme
TNSDL—for developing telecommunication exchanges, uses asynchronous message passing
VHDL (VHSIC Hardware Description Language)—IEEE STD-1076
XC—concurrency-extended subset of C language developed by XMOS, based on Communicating Sequential Processes, built-in constructs for programmable I/O

Many other languages provide support for concurrency in the form of libraries, at levels roughly comparable with the above list.
See also[edit]

List of important publications in concurrent, parallel, and distributed computing
Chu space
Critical section
Flow-based programming
Multitasking
Parallel computing
Ptolemy Project
Race condition
Sheaf (mathematics)
Software transactional memory
Transaction processing

Notes[edit]


^ This is discounting parallelism internal to a processor core, such as pipelining or vectorized instructions. A single-core, single-processor machine may be capable of some parallelism, such as with a coprocessor, but the processor itself is not.


References[edit]





This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (February 2014)




^ Operating System Concepts 9th edition, Abraham Silberschatz. "Chapter 4: Threads"
^ a b "Concurrency is not Parallelism", Waza conference Jan 11, 2012, Rob Pike (slides) (video)
^ "Parallelism vs. Concurrency". Haskell Wiki.&#160;
^ Schneider, Fred B. On Concurrent Programming. Springer. ISBN&#160;9780387949420.&#160;
^ a b Ben-Ari, Mordechai (2006). Principles of Concurrent and Distributed Programming (2nd ed.). Addison-Wesley. ISBN&#160;978-0-321-31283-9.&#160;
^ Patterson &amp; Hennessy 2013, p.&#160;503.
^ "PODC Influential Paper Award: 2002", ACM Symposium on Principles of Distributed Computing, retrieved 2009-08-24&#160;
^ Armstrong, Joe (2003). "Making reliable distributed systems in the presence of software errors".&#160; Missing or empty |url= (help); |access-date= requires |url= (help)
^ Blum, Ben (2012). "Typesafe Shared Mutable State". Retrieved 2012-11-14.&#160;




Patterson, David A.; Hennessy, John L. (2013). Computer Organization and Design: The Hardware/Software Interface. The Morgan Kaufmann Series in Computer Architecture and Design (5 ed.). Morgan Kaufmann. ISBN&#160;978-0-12407886-4.&#160;


Further reading[edit]

Dijkstra, E. W. (1965). "Solution of a problem in concurrent programming control". Communications of the ACM 8 (9): 569. doi:10.1145/365559.365617.&#160;
Herlihy, Maurice (2008) [2008]. The Art of Multiprocessor Programming. Morgan Kaufmann. ISBN&#160;978-0123705914.&#160;
Downey, Allen B. (2005) [2005]. The Little Book of Semaphores (PDF). Green Tea Press. ISBN&#160;1-4414-1868-7.&#160;
Filman, Robert E.; Daniel P. Friedman (1984). Coordinated Computing: Tools and Techniques for Distributed Software. New York: McGraw-Hill. p.&#160;370. ISBN&#160;0-07-022439-0.&#160;
Leppäjärvi, Jouni (2008). A pragmatic, historically oriented survey on the universality of synchronization primitives (PDF). University of Oulu.&#160;
Taubenfeld, Gadi (2006). Synchronization Algorithms and Concurrent Programming. Pearson / Prentice Hall. p.&#160;433. ISBN&#160;0-13-197259-6.&#160;

External links[edit]

Concurrent Systems Virtual Library









v
t
e


Concurrent computing






General



Concurrency
Concurrency control








Process calculi



CSP
CCS
ACP
LOTOS
π-calculus
Ambient calculus
API-Calculus
PEPA
Join-calculus











&#160;Category: Concurrent computing
















v
t
e


Types of programming languages









Array
Aspect-oriented
Class-based
Concatenative
Concurrent
Data-structured
Dataflow
Declarative
Domain-specific
Dynamic
Esoteric
Event-driven
Extensible
Functional
Imperative
Logic
Macro
Metaprogramming
Multi-paradigm
Object-based
Object-oriented
Pipeline
Procedural
Prototype-based
Reflective
Rule-based
Scripting
Synchronous
Templating











Assembly
Compiled
Interpreted
Machine











Low-level
High-level
Very high-level











First generation
Second generation
Third generation
Fourth generation
Fifth generation











Non-English-based
Off-side rule
Visual















					
						Retrieved from "https://en.wikipedia.org/w/index.php?title=Concurrent_computing&amp;oldid=684064233"					
				Categories: Operating system technologyConcurrent computingHidden categories: Pages using web citations with no URLPages using citations with accessdate and no URLArticles needing additional references from December 2006All articles needing additional referencesArticles to be expanded from February 2014All articles to be expandedAll articles with unsourced statementsArticles with unsourced statements from May 2013Articles with unsourced statements from August 2010Articles needing additional references from February 2014				
							
		
		
			