



Pattern recognition - Wikipedia, the free encyclopedia
document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );
window.RLQ = window.RLQ || []; window.RLQ.push( function () {
mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Pattern_recognition","wgTitle":"Pattern recognition","wgCurRevisionId":688980762,"wgRevisionId":688980762,"wgArticleId":126706,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Articles needing additional references from November 2014","All articles needing additional references","Accuracy disputes from May 2014","All accuracy disputes","Articles with disputed statements from November 2014","All articles with unsourced statements","Articles with unsourced statements from November 2014","Articles to be split from May 2014","All articles to be split","Articles with unsourced statements from January 2011","Articles needing cleanup from May 2014","All pages needing cleanup","Wikipedia list cleanup from May 2014","Articles with unsourced statements from May 2014","Wikipedia articles with GND identifiers","Machine learning","Formal sciences","Pattern recognition"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Pattern_recognition","wgRelevantArticleId":126706,"wgIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wikilove-recipient":"","wikilove-anon":0,"wgWikiEditorEnabledModules":{"toolbar":true,"dialogs":true,"preview":false,"publish":false},"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","usePageImages":true,"usePageDescriptions":true},"wgGatherShouldShowTutorial":true,"wgGatherEnableSample":0,"wgGatherPageImageThumbnail":"//upload.wikimedia.org/wikipedia/commons/thumb/d/d9/800px-Cool_Kids_of_Death_Off_Festival_p_146-face_selected.jpg/100px-800px-Cool_Kids_of_Death_Off_Festival_p_146-face_selected.jpg","wgULSAcceptLanguageList":["en-us","en"],"wgULSCurrentAutonym":"English","wgFlaggedRevsParams":{"tags":{"status":{"levels":1,"quality":2,"pristine":3}}},"wgStableRevisionId":null,"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgNoticeProject":"wikipedia","wgCentralNoticeCategoriesUsingLegacy":["Fundraising"],"wgWikibaseItemId":"Q378859","wgVisualEditorToolbarScrollOffset":0}); /* @nomin */mw.loader.implement("user.options",function($,jQuery){mw.user.options.set({"variant":"en"});});mw.loader.implement("user.tokens",function ( $, jQuery ) {
mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\"}); /* @nomin */ ;

});mw.loader.load(["mediawiki.page.startup","mediawiki.legacy.wikibits","ext.centralauth.centralautologin","ext.gadget.WatchlistBase","ext.gadget.WatchlistGreenIndicators","mmv.head","ext.visualEditor.desktopArticleTarget.init","ext.uls.init","ext.uls.interface","ext.quicksurveys.init","ext.centralNotice.bannerController","skins.vector.js"]);
} );



a:lang(ar),a:lang(kk-arab),a:lang(mzn),a:lang(ps),a:lang(ur){text-decoration:none}













body{behavior:url("/w/static/1.27.0-wmf.7/skins/Vector/csshover.min.htc")}


		
		
		
			

							
						

			Pattern recognition
									
									From Wikipedia, the free encyclopedia
								
												
					Jump to:					navigation, 					search
				
				




This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (November 2014)







This article's factual accuracy is disputed. Please help to ensure that disputed statements are reliably sourced. See the relevant discussion on the talk page. (May 2014)


This article is about pattern recognition as a branch of machine learning.  For the cognitive process, see Pattern recognition (psychology).  For other uses, see Pattern recognition (disambiguation).


Machine learning and
data mining





Problems





Classification
Clustering
Regression
Anomaly detection
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction






Supervised learning
(classification&#160;• regression)






Decision trees
Ensembles (Bagging, Boosting, Random forest)
k-NN
Linear regression
Naive Bayes
Neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)





Clustering





BIRCH
Hierarchical
k-means
Expectation-maximization (EM)

DBSCAN
OPTICS
Mean-shift





Dimensionality reduction





Factor analysis
CCA
ICA
LDA
NMF
PCA
t-SNE





Structured prediction





Graphical models (Bayes net, CRF, HMM)





Anomaly detection





k-NN
Local outlier factor





Neural nets





Autoencoder
Deep learning
Multilayer perceptron
RNN
Restricted Boltzmann machine
SOM
Convolutional neural network





Theory





Bias-variance dilemma
Computational learning theory
Empirical risk minimization
PAC learning
Statistical learning
VC theory





Conferences and Journals





NIPS
ICML
JMLR
KDD
ICDM
SDM







 Machine learning portal







v
t
e





Pattern recognition is a branch of machine learning that focuses on the recognition of patterns and regularities in data, although it is in some cases considered to be nearly synonymous with machine learning.[1] Pattern recognition systems are in many cases trained from labeled "training" data (supervised learning), but when no labeled data are available other algorithms can be used to discover previously unknown patterns (unsupervised learning).
The terms pattern recognition, machine learning, data mining and knowledge discovery in databases (KDD) are hard to separate, as they largely overlap in their scope. Machine learning is the common term for supervised learning methods[dubious – discuss] and originates from artificial intelligence, whereas KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition has its origins in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition. In pattern recognition, there may be a higher interest to formalize, explain and visualize the pattern, while machine learning traditionally focuses on maximizing the recognition rates. Yet, all of these domains have evolved substantially from their roots in artificial intelligence, engineering and statistics, and they've become increasingly similar by integrating developments and ideas from each other.
In machine learning, pattern recognition is the assignment of a label to a given input value. In statistics, discriminant analysis was introduced for this same purpose in 1936. An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is "spam" or "non-spam"). However, pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence.[citation needed]
Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform "most likely" matching of the inputs, taking into account their statistical variation. This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is regular expression matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors. In contrast to pattern recognition, pattern matching is generally not considered a type of machine learning, although pattern-matching algorithms (especially with fairly general, carefully tailored patterns) can sometimes succeed in providing similar-quality output of the sort provided by pattern-recognition algorithms.



Contents


1 Overview

1.1 Probabilistic classifiers
1.2 How many feature variables are important?


2 Problem statement (supervised version)

2.1 Frequentist or Bayesian approach to pattern recognition?


3 Uses
4 Algorithms

4.1 Classification algorithms (supervised algorithms predicting categorical labels)
4.2 Clustering algorithms (unsupervised algorithms predicting categorical labels)
4.3 Ensemble learning algorithms (supervised meta-algorithms for combining multiple learning algorithms together)
4.4 General algorithms for predicting arbitrarily-structured (sets of) labels
4.5 Multilinear subspace learning algorithms (predicting labels of multidimensional data using tensor representations)
4.6 Real-valued sequence labeling algorithms (predicting sequences of real-valued labels)
4.7 Regression algorithms (predicting real-valued labels)
4.8 Sequence labeling algorithms (predicting sequences of categorical labels)


5 See also
6 References
7 Further reading
8 External links



Overview[edit]
Pattern recognition is generally categorized according to the type of learning procedure used to generate the output value. Supervised learning assumes that a set of training data (the training set) has been provided, consisting of a set of instances that have been properly labeled by hand with the correct output. A learning procedure then generates a model that attempts to meet two sometimes conflicting objectives: Perform as well as possible on the training data, and generalize as well as possible to new data (usually, this means being as simple as possible, for some technical definition of "simple", in accordance with Occam's Razor, discussed below). Unsupervised learning, on the other hand, assumes training data that has not been hand-labeled, and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances.[2] A combination of the two that has recently been explored is semi-supervised learning, which uses a combination of labeled and unlabeled data (typically a small set of labeled data combined with a large amount of unlabeled data). Note that in cases of unsupervised learning, there may be no training data at all to speak of; in other words, the data to be labeled is the training data.
Note that sometimes different terms are used to describe the corresponding supervised and unsupervised learning procedures for the same type of output. For example, the unsupervised equivalent of classification is normally known as clustering, based on the common perception of the task as involving no training data to speak of, and of grouping the input data into clusters based on some inherent similarity measure (e.g. the distance between instances, considered as vectors in a multi-dimensional vector space), rather than assigning each input instance into one of a set of pre-defined classes. Note also that in some fields, the terminology is different: For example, in community ecology, the term "classification" is used to refer to what is commonly known as "clustering".
The piece of input data for which an output value is generated is formally termed an instance. The instance is formally described by a vector of features, which together constitute a description of all known characteristics of the instance. (These feature vectors can be seen as defining points in an appropriate multidimensional space, and methods for manipulating vectors in vector spaces can be correspondingly applied to them, such as computing the dot product or the angle between two vectors.) Typically, features are either categorical (also known as nominal, i.e., consisting of one of a set of unordered items, such as a gender of "male" or "female", or a blood type of "A", "B", "AB" or "O"), ordinal (consisting of one of a set of ordered items, e.g., "large", "medium" or "small"), integer-valued (e.g., a count of the number of occurrences of a particular word in an email) or real-valued (e.g., a measurement of blood pressure). Often, categorical and ordinal data are grouped together; likewise for integer-valued and real-valued data. Furthermore, many algorithms work only in terms of categorical data and require that real-valued or integer-valued data be discretized into groups (e.g., less than 5, between 5 and 10, or greater than 10).
Probabilistic classifiers[edit]
Main article: Probabilistic classifier
‹ The template below (Split section portions) is being considered for deletion. See templates for discussion to help reach a consensus.›





It has been suggested that portions of this section be split into a new article titled Probabilistic classifier. (Discuss) (May 2014)


Many common pattern recognition algorithms are probabilistic in nature, in that they use statistical inference to find the best label for a given instance. Unlike other algorithms, which simply output a "best" label, often probabilistic algorithms also output a probability of the instance being described by the given label. In addition, many probabilistic algorithms output a list of the N-best labels with associated probabilities, for some value of N, instead of simply a single best label. When the number of possible labels is fairly small (e.g., in the case of classification), N may be set so that the probability of all possible labels is output. Probabilistic algorithms have many advantages over non-probabilistic algorithms:

They output a confidence value associated with their choice. (Note that some other algorithms may also output confidence values, but in general, only for probabilistic algorithms is this value mathematically grounded in probability theory. Non-probabilistic confidence values can in general not be given any specific meaning, and only used to compare against other confidence values output by the same algorithm.)
Correspondingly, they can abstain when the confidence of choosing any particular output is too low.
Because of the probabilities output, probabilistic pattern-recognition algorithms can be more effectively incorporated into larger machine-learning tasks, in a way that partially or completely avoids the problem of error propagation.

How many feature variables are important?[edit]
Feature selection algorithms attempt to directly prune out redundant or irrelevant features. A general introduction to feature selection which summarizes approaches and challenges, has been given.[3] The complexity of feature-selection is, because of its non-monotonous character, an optimization problem where given a total of  features the powerset consisting of all  subsets of features need to be explored. The Branch-and-Bound algorithm [4] does reduce this complexity but is intractable for medium to large values of the number of available features . For a large-scale comparison of feature-selection algorithms see .[5]
Techniques to transform the raw feature vectors (feature extraction) are sometimes used prior to application of the pattern-matching algorithm. For example, feature extraction algorithms attempt to reduce a large-dimensionality feature vector into a smaller-dimensionality vector that is easier to work with and encodes less redundancy, using mathematical techniques such as principal components analysis (PCA). The distinction between feature selection and feature extraction is that the resulting features after feature extraction has taken place are of a different sort than the original features and may not easily be interpretable, while the features left after feature selection are simply a subset of the original features.
Problem statement (supervised version)[edit]
Formally, the problem of supervised pattern recognition can be stated as follows: Given an unknown function  (the ground truth) that maps input instances  to output labels , along with training data  assumed to represent accurate examples of the mapping, produce a function  that approximates as closely as possible the correct mapping . (For example, if the problem is filtering spam, then  is some representation of an email and  is either "spam" or "non-spam"). In order for this to be a well-defined problem, "approximates as closely as possible" needs to be defined rigorously. In decision theory, this is defined by specifying a loss function that assigns a specific value to "loss" resulting from producing an incorrect label. The goal then is to minimize the expected loss, with the expectation taken over the probability distribution of . In practice, neither the distribution of  nor the ground truth function  are known exactly, but can be computed only empirically by collecting a large number of samples of  and hand-labeling them using the correct value of  (a time-consuming process, which is typically the limiting factor in the amount of data of this sort that can be collected). The particular loss function depends on the type of label being predicted. For example, in the case of classification, the simple zero-one loss function is often sufficient. This corresponds simply to assigning a loss of 1 to any incorrect labeling and implies that the optimal classifier minimizes the error rate on independent test data (i.e. counting up the fraction of instances that the learned function  labels wrongly, which is equivalent to maximizing the number of correctly classified instances). The goal of the learning procedure is then to minimize the error rate (maximize the correctness) on a "typical" test set.
For a probabilistic pattern recognizer, the problem is instead to estimate the probability of each possible output label given a particular input instance, i.e., to estimate a function of the form



where the feature vector input is , and the function f is typically parameterized by some parameters .[6] In a discriminative approach to the problem, f is estimated directly. In a generative approach, however, the inverse probability  is instead estimated and combined with the prior probability  using Bayes' rule, as follows:



When the labels are continuously distributed (e.g., in regression analysis), the denominator involves integration rather than summation:



The value of  is typically learned using maximum a posteriori (MAP) estimation. This finds the best value that simultaneously meets two conflicting objects: To perform as well as possible on the training data (smallest error-rate) and to find the simplest possible model. Essentially, this combines maximum likelihood estimation with a regularization procedure that favors simpler models over more complex models. In a Bayesian context, the regularization procedure can be viewed as placing a prior probability  on different values of . Mathematically:



where  is the value used for  in the subsequent evaluation procedure, and , the posterior probability of , is given by



In the Bayesian approach to this problem, instead of choosing a single parameter vector , the probability of a given label for a new instance  is computed by integrating over all possible values of , weighted according to the posterior probability:



Frequentist or Bayesian approach to pattern recognition?[edit]
The first pattern classifier – the linear discriminant presented by Fisher – was developed in the Frequentist tradition. The frequentist approach entails that the model parameters are considered unknown, but objective. The parameters are then computed (estimated) from the collected data. For the linear discriminant, these parameters are precisely the mean vectors and the Covariance matrix. Also the probability of each class  is estimated from the collected dataset. Note that the usage of ‘Bayes rule’ in a pattern classifier does not make the classification approach Bayesian.
Bayesian statistics has its origin in Greek philosophy where a distinction was already made between the ‘a priori’ and the ‘a posteriori’ knowledge. Later Kant defined his distinction between what is a priori known – before observation – and the empirical knowledge gained from observations. In a Bayesian pattern classifier, the class probabilities  can be chosen by the user, which are then a priori. Moreover, experience quantified as a priori parameter values can be weighted with empirical observations – using e.g., the Beta- (conjugate prior) and Dirichlet-distributions. The Bayesian approach facilitates a seamless intermixing between expert knowledge in the form of subjective probabilities, and objective observations.
Probabilistic pattern classifiers can be used according to a frequentist or a Bayesian approach.
Uses[edit]




The face was automatically detected by special software.


Within medical science, pattern recognition is the basis for computer-aided diagnosis (CAD) systems. CAD describes a procedure that supports the doctor's interpretations and findings.




Pattern &amp; Shape Recognition Technology (SRT) in a people counter system


Other typical applications of pattern recognition techniques are automatic speech recognition, classification of text into several categories (e.g., spam/non-spam email messages), the automatic recognition of handwritten postal codes on postal envelopes, automatic recognition of images of human faces, or handwriting image extraction from medical forms.[7] The last two examples form the subtopic image analysis of pattern recognition that deals with digital images as input to pattern recognition systems.[8][9]
Optical character recognition is a classic example of the application of a pattern classifier, see OCR-example. The method of signing one's name was captured with stylus and overlay starting in 1990.[citation needed] The strokes, speed, relative min, relative max, acceleration and pressure is used to uniquely identify and confirm identity. Banks were first offered this technology, but were content to collect from the FDIC for any bank fraud and did not want to inconvenience customers..[citation needed]
Artificial neural networks (neural net classifiers) and Deep Learning have many real-world applications in image processing, a few examples:

identification and authentication: e.g., license plate recognition,[10] fingerprint analysis and face detection/verification;[11]
medical diagnosis: e.g., screening for cervical cancer (Papnet)[12] or breast tumors;
defence: various navigation and guidance systems, target recognition systems, shape recognition technology etc.

For a discussion of the aforementioned applications of neural networks in image processing, see e.g.[13]
In psychology, pattern recognition, making sense of and identifying the objects we see is closely related to perception, which explains how the sensory inputs we receive are made meaningful. Pattern recognition can be thought of in two different ways: the first being template matching and the second being feature detection. A template is a pattern used to produce items of the same proportions. The template-matching hypothesis suggests that incoming stimuli are compared with templates in the long term memory. If there is a match, the stimulus is identified. Feature detection models, such as the Pandemonium system for classifying letters (Selfridge, 1959), suggest that the stimuli are broken down into their component parts for identification. For example, a capital E has three horizontal lines and one vertical line.[14]
Algorithms[edit]
Algorithms for pattern recognition depend on the type of label output, on whether learning is supervised or unsupervised, and on whether the algorithm is statistical or non-statistical in nature. Statistical algorithms can further be categorized as generative or discriminative.





This article contains embedded lists that may be poorly defined, unverified or indiscriminate. Please help to clean it up to meet Wikipedia's quality standards. Where appropriate, incorporate items into the main body of the article. (May 2014)


Classification algorithms (supervised algorithms predicting categorical labels)[edit]
Main article: Statistical classification
Parametric:[15]

Linear discriminant analysis
Quadratic discriminant analysis
Maximum entropy classifier (aka logistic regression, multinomial logistic regression): Note that logistic regression is an algorithm for classification, despite its name. (The name comes from the fact that logistic regression uses an extension of a linear regression model to model the probability of an input being in a particular class.)

Nonparametric:[16]

Decision trees, decision lists
Kernel estimation and K-nearest-neighbor algorithms
Naive Bayes classifier
Neural networks (multi-layer perceptrons)
Perceptrons
Support vector machines
Gene expression programming

Clustering algorithms (unsupervised algorithms predicting categorical labels)[edit]
Main article: Cluster analysis

Categorical mixture models
Deep learning methods[citation needed]
Hierarchical clustering (agglomerative or divisive)
K-means clustering
Correlation clustering
Kernel principal component analysis (Kernel PCA)

Ensemble learning algorithms (supervised meta-algorithms for combining multiple learning algorithms together)[edit]
Main article: Ensemble learning

Boosting (meta-algorithm)
Bootstrap aggregating ("bagging")
Ensemble averaging
Mixture of experts, hierarchical mixture of experts

General algorithms for predicting arbitrarily-structured (sets of) labels[edit]

Bayesian networks
Markov random fields

Multilinear subspace learning algorithms (predicting labels of multidimensional data using tensor representations)[edit]
Unsupervised:

Multilinear principal component analysis (MPCA)

Real-valued sequence labeling algorithms (predicting sequences of real-valued labels)[edit]
Main article: sequence labeling
Supervised (?):

Kalman filters
Particle filters

Regression algorithms (predicting real-valued labels)[edit]
Main article: Regression analysis
Supervised:

Gaussian process regression (kriging)
Linear regression and extensions
Neural networks and Deep learning methods

Unsupervised:

Independent component analysis (ICA)
Principal components analysis (PCA)

Sequence labeling algorithms (predicting sequences of categorical labels)[edit]
Supervised:

Conditional random fields (CRFs)
Hidden Markov models (HMMs)
Maximum entropy Markov models (MEMMs)
Recurrent neural networks

Unsupervised:

Hidden Markov models (HMMs)

See also[edit]


Adaptive resonance theory
Cache language model
Compound term processing
Computer-aided diagnosis
Data mining
Deep Learning
List of numerical analysis software
List of numerical libraries
Machine learning
Multilinear subspace learning
Neocognitron
Perception
Perceptual learning
Predictive analytics
Prior knowledge for pattern recognition
Sequence mining
Template matching
Contextual image classification


References[edit]
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.


^ Bishop, Christopher M. (2006). Pattern Recognition and Machine Learning. Springer. p.&#160;vii. Pattern recognition has its origins in engineering, whereas machine learning grew out of computer science. However, these activities can be viewed as two facets of the same field, and together they have undergone substantial development over the past ten years.&#160;
^ Carvalko, J.R., Preston K. (1972). "On Determining Optimum Simple Golay Marking Transforms for Binary Image Processing". IEEE Transactions on Computers 21: 1430–33. doi:10.1109/T-C.1972.223519.&#160;.
^ Isabelle Guyon Clopinet, André Elisseeff (2003). An Introduction to Variable and Feature Selection. The Journal of Machine Learning Research, Vol. 3, 1157-1182. Link
^ Iman Foroutan, Jack Sklansky (1987). "Feature Selection for Automatic Classification of Non-Gaussian Data". IEEE Transactions on Systems, Man and Cybernetics 17 (2): 187–198. doi:10.1109/TSMC.1987.4309029.&#160;.
^ Mineichi Kudo, Jack Sklansky (2000). "Comparison of algorithms that select features for pattern classifiers". Pattern Recognition 33 (1): 25–41. doi:10.1016/S0031-3203(99)00041-2.&#160;.
^ For linear discriminant analysis the parameter vector  consists of the two mean vectors  and  and the common covariance matrix .
^ Milewski, Robert; Govindaraju, Venu (31 March 2008). "Binarization and cleanup of handwritten text from carbon copy medical form images". Pattern Recognition 41 (4): 1308–1315. doi:10.1016/j.patcog.2007.08.018.&#160;
^ Richard O. Duda, Peter E. Hart, David G. Stork (2001) Pattern classification (2nd edition), Wiley, New York, ISBN 0-471-05669-3
^ R. Brunelli, Template Matching Techniques in Computer Vision: Theory and Practice, Wiley, ISBN 978-0-470-51706-2, 2009
^ THE AUTOMATIC NUMBER PLATE RECOGNITION TUTORIAL http://anpr-tutorial.com/
^ Neural Networks for Face Recognition Companion to Chapter 4 of the textbook Machine Learning.
^ PAPNET For Cervical Screening http://health-asia.org/papnet-for-cervical-screening/
^ Egmont-Petersen, M., de Ridder, D., Handels, H. (2002). "Image processing with neural networks - a review". Pattern Recognition 35 (10): 2279–2301. doi:10.1016/S0031-3203(01)00178-9.&#160;
^ "A-level Psychology Attention Revision - Pattern recognition | S-cool, the revision website". S-cool.co.uk. Retrieved 2012-09-17.&#160;
^ Assuming known distributional shape of feature distributions per class, such as the Gaussian shape.
^ No distributional assumption regarding shape of feature distributions per class.


Further reading[edit]

Fukunaga, Keinosuke (1990). Introduction to Statistical Pattern Recognition (2nd ed.). Boston: Academic Press. ISBN&#160;0-12-269851-7.&#160;
Koutroumbas, Konstantinos; Theodoridis, Sergios (2008). Pattern Recognition (4th ed.). Boston: Academic Press. ISBN&#160;1-59749-272-8.&#160;
Hornegger, Joachim; Paulus, Dietrich W. R. (1999). Applied Pattern Recognition: A Practical Introduction to Image and Speech Processing in C++ (2nd ed.). San Francisco: Morgan Kaufmann Publishers. ISBN&#160;3-528-15558-2.&#160;
Schuermann, Juergen (1996). Pattern Classification: A Unified View of Statistical and Neural Approaches. New York: Wiley. ISBN&#160;0-471-13534-8.&#160;
Godfried T. Toussaint, ed. (1988). Computational Morphology. Amsterdam: North-Holland Publishing Company.&#160;
Kulikowski, Casimir A.; Weiss, Sholom M. (1991). Computer Systems That Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems. Machine Learning. San Francisco: Morgan Kaufmann Publishers. ISBN&#160;1-55860-065-5.&#160;
Jain, Anil.K.; Duin, Robert.P.W.; Mao, Jianchang (2000). "Statistical pattern recognition: a review". IEEE Transactions on Pattern Analysis and Machine Intelligence 22 (1): 4–37. doi:10.1109/34.824819.&#160;
An introductory tutorial to classifiers (introducing the basic terms, with numeric example)

External links[edit]

The International Association for Pattern Recognition
List of Pattern Recognition web sites
Journal of Pattern Recognition Research
Pattern Recognition Info
Pattern Recognition (Journal of the Pattern Recognition Society)
International Journal of Pattern Recognition and Artificial Intelligence
International Journal of Applied Pattern Recognition
Open Pattern Recognition Project, intended to be an open source platform for sharing algorithms of pattern recognition









v
t
e


Sub-disciplines of computing









Hardware
Software
Firmware








Information system



Information security
Multimedia database
Intelligent database
Big data
Knowledge-based systems
Recommender System
Geographic information systems
Decision support system
Data engineering
Knowledge engineering
Fuzzy logic
Data analysis
System analysis and design
Project management
Knowledge management
Data integration
High-performance computing
Semantic web








Computer science



Database management system
Communications system
Computational geometry
Machine learning
Data mining
Parallel programming
Coding theory
Theory of computation
Constraint programming
Computational biology (Bioinformatics)
Design and analysis of algorithms
Information retrieval
Computational science
Symbolic computation
Distributed computing
Evolutionary computation
Natural computation
Combinatorial optimization
Parallel processing








Computer engineering



Multimedia
Satellite navigation (GNSS)
Embedded systems
Real-time computing
Computer architecture
Pair programming
System programming
Neural networks
Speech recognition
Signal analysis
Computer vision (Visual computing)
IC Design
Voice over IP
Speech synthesis
Human-Computer Interaction
Microprocessor
Image processing
Natural language processing
Speech processing
Digital signal processing








Software engineering



Software maintenance
Formal methods
Software quality
Software quality assurance
Measurement Software
Fault-tolerant software
Software testing
Enterprise architecture
Software architecture
Software engineering economics
Agile software development
Software design pattern
Software modelling
Systems analyst
Object-oriented analysis and design (UML)
Requirements analysis
Software development
Software configuration management
Software project management
Software engineering management
Software development process (Software release life cycle)
Software design
Software deployment
Software enhancement








Computer network



Network security
Secure electronic transaction
Network Performance Evaluation (QoS)
Cloud computing
Routing
Distributed systems (Distributed database)
Information theory
Wireless network
Next-generation network
Cellular network
Optical transport network (Optical networking)
Cryptography
Network simulation
Pattern recognition
Network administrators
Network equipment
Network design
Ubiquitous and Mobile computing
Data center
Mobile communications
Digital communication
Communications satellite
Telecommunication (Telecommunications network)








Business informatics



ITIL
ITSM
Enterprise resource planning
Electronic business
Business intelligence
Human Resources Development
Security management
Configuration management
Technology management
Power management
Service management
Project management
Systems management
Network management
Content management
Customer relationship management
Incident management
Asset management
Integrated management
Communications management
System administrator













Authority control



GND: 4040936-3
NDL: 00569072















					
						Retrieved from "https://en.wikipedia.org/w/index.php?title=Pattern_recognition&amp;oldid=688980762"					
				Categories: Machine learningFormal sciencesPattern recognitionHidden categories: Articles needing additional references from November 2014All articles needing additional referencesAccuracy disputes from May 2014All accuracy disputesArticles with disputed statements from November 2014All articles with unsourced statementsArticles with unsourced statements from November 2014Articles to be split from May 2014All articles to be splitArticles with unsourced statements from January 2011Articles needing cleanup from May 2014All pages needing cleanupWikipedia list cleanup from May 2014Articles with unsourced statements from May 2014Wikipedia articles with GND identifiers				
							
		
		
			Navigation menu

			
									
						Personal tools
						
							Create accountLog in						
					
									
										
						Namespaces
						
															Article
															Talk
													
					
										
												
							Variants
						

						
							
															
						
					
									
				
										
						Views
						
															Read
															Edit
															View history
													
					
										
						More

						
							
															
						
					
										
						
							Search
						

						
							
														
						
					
									
			
			
				
						
			Navigation

			
									
						Main pageContentsFeatured contentCurrent eventsRandom articleDonate to WikipediaWikipedia store					
							
		
			
			Interaction

			
									
						HelpAbout WikipediaCommunity portalRecent changesContact page					
							
		
			
			Tools

			
									
						What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationWikidata itemCite this page					
							
		
			
			Print/export

			
									
						Create a bookDownload as PDFPrintable version					
							
		
			
			Languages

			
									
						العربيةCatalàDeutschΕλληνικάEspañolفارسیFrançais한국어Bahasa IndonesiaItalianoBasa JawaҚазақшаLietuviųമലയാളംBahasa MelayuNederlands日本語PolskiPortuguêsРусскийСрпски / srpskiSrpskohrvatski / српскохрватскиSuomiTagalogไทยTürkçeУкраїнськаTiếng Việt中文					
				Edit links			
		
				
		
		
							
											 This page was last modified on 4 November 2015, at 05:29.
											Text is available under the Creative Commons Attribution-ShareAlike License;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
									
							
											Privacy policy
											About Wikipedia
											Disclaimers
											Contact Wikipedia
											Developers
											